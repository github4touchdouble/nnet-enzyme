\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}

\begin{document}
\firstpage{1}

\subtitle{Subject Section}

\title[References]{This is a title}
\author[Sample \textit{et~al}.]{Corresponding Author\,$^{\text{\sfb 1,}*}$, Co-Author\,$^{\text{\sfb 2}}$ and Co-Author\,$^{\text{\sfb 2,}*}$}
\address{$^{\text{\sf 1}}$Department, Institution, City, Post Code, Country and \\
$^{\text{\sf 2}}$Department, Institution, City, Post Code,
Country.}

\corresp{$^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} \\
	\textbf{Results:}\\
\textbf{Availability:} \\
\textbf{Contact:} \href{name@bio.com}{name@bio.com}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle

\section{Abstract}
%% Malte
The accurate prediction of enzyme commission numbers (EC numbers) is not only crucial for 
the classification and understanding of newly discovered enzymes but also for completing the annotation of already known enzymes.
Therefore, developing a reliable method for predicting EC numbers is of great importance.

However, due to insufficient data, enzyme function prediction using machine learning is an ongoing challenge.
In this paper, we propose several methods for predicting enzymes in three different problem categories (Table~\ref{Tab:01}).
Throughout the developing of our models, we used a variety of different input features and machine learning algorithms, of which the best will be thoroughly reviewed in this paper.
\begin{center}
\begin{table}[!htbp]
\processtable{Description of subproblem categories\label{Tab:01}} {\begin{tabular}{@{}llll@{}}\toprule 
		Level & Description & Best performing method & F1 score\\\midrule
		0 & Binary classification & Random Forest & score\\
		1 & Main class classification & Feedforward neuronal network & score \\
		2 & Subclass classification & Feedforward neuronal network & score \\\botrule
\end{tabular}}{}
\end{table}
\end{center}




\section{Introduction}

Enzymes play a vital role as biological catalysts, facilitating essential biochemical reactions in organisms.
Without enzyme catalysis, these reactions would either occur too slowly or be practically impossible.
Predominantly proteins, enzymes are designated specific functions; for instance, Oxidoreductases drive redox reactions, and Isomerases convert molecules into their isomers.
Therefore, a comprehensive understanding and a precise classification of enzymes are fundamental.

In the past, scientists attempted to categorize enzymes into groups and develop a logical rule set for naming.
However, the efforts were hindered by ambiguity.
A significant milestone occurred in 1956 with the establishment of an official international commission on enzyme classification (\cite{IUBMB}).
This marked the initiation of the contemporary enzyme classification system that forms the basis for our understanding of enzymes today.

In modern research, computational methods have become invaluable for enzyme classification; this shift has transformed the traditionally labor-intensive process.
Machine learning and data-driven models, in particular, have assumed a leading role, holding the potential for enhanced accuracy and efficiency in annotating enzymes within vast genomic data.

Several effective methods have been developed to address this challenge.
One notable example is DeepEC (\cite{DeepEC}), a deep learning-based computational framework designed for the precise and high-throughput prediction of EC numbers for protein sequences.
DeepEC employs three convolutional neural networks (CNNs) as a primary engine for EC number prediction and incorporates homology analysis for cases not classified by the CNNs.

\begin{methods}
\section{Methods}
The protein data was taken from the UniProt database and used in the CLEAN publication on Enzyme function prediction (see \cite{CleanArticle}). 
The mass table of common amino acids was taken from \cite{BioinformaticsSolutionsInc}. 

\subsection{Machine learning algorithms used per level}

Our initial goal is to categorize proteins as either enzymes or non-enzymes (level 0). 
To accomplish this, we've opted for the Random Forest, a straightforward yet powerful machine learning method.
The choice of Random Forest is driven by its effectiveness in handling classification tasks, making it a well-suited option for our specific protein classification objective.
We built a Random Forest Classifier using the scikit-learn library with specific parameters, which will be explained in the training procedure. 
%% Malte
For ... 

For ...

\subsection{Data preprocessing in general}
% TODO:malte
Unwanted Sequences Removal
We excluded sequences that contain the amino acids "O" and "U" from our dataset. 



\subsection{Data preprocessing for level 0}
As a random forest model relies on multiple decision trees, and each decision tree requires various features,
we opted to extract additional information from both the protein sequence and the esm2 embeddings. %%FIX: embeddings

From the amino acid mass table we computed the mass of protein sequences by adding up the individual masses of their amino acid components.
The esm2 embeddings, each represented by a $2560$-dimensional vector, underwent statistical analysis.
We computed the median, standard deviation, and vector magnitude by aggregating values across all $2560$ dimensions for each protein.
To simplify the embeddings, we applied Principal Component Analysis (\cite{scikit-learn})
separately to enzyme and non-enzyme datasets,
retaining $90\%$ of the variance. This process yielded reduced dimensions of $397$ for enzymes and $369$ for non-enzymes. 
Therefore, we reduced the dimensions to $397$, providing a streamlined representation of the protein embeddings while retaining crucial information for both 
enzyme (see figure \ref{fig:01}) and non-enzyme (see figure \ref{fig:02}) datasets.

\begin{figure}[!bp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_1.jpeg}
\caption{The number of components needed to explain the variance in the enzyme dataset}\label{fig:01}
\end{figure}

\begin{figure}[!tbp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_2.jpeg}
\caption{The number of components needed to explain the variance in the non enzyme dataset}\label{fig:02}
\end{figure}


By combining the information from the proteins’ sequences, masses, and embeddings, we created a Pandas DataFrame that concatenates enzymes and non-enzymes,
including $401$ features: mass, embeddings median, embeddings standard deviation (std), embeddings magnitude, and dimension $1$ to $397$ of the reduced 
embeddings. 


%% Fig 3
\begin{figure}[!htbp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_3.jpeg}
\caption{The number of components needed to explain the variance in the non enzyme dataset}\label{fig:03}
\end{figure}

We also used the method SelectFromModel from the scikit-learn library (see \cite{scikit-learn})
to select features based on importance weights. 
The refined set of input features contains 
'Mass,' 'Emb median,' 'Emb std,' 'Emb magnitude,' and 'PCA 1' through 'PCA 47' (see figure \ref{fig:03}) excluding ‘PCA 29’, ‘PCA 31’, ‘PCA 35’, 
‘PCA 36’, ‘PCA 39’ to ‘PCA 42’, ‘PCA 44’ and ‘PCA 46’ (where 'Emb' represents embeddings and 'PCA X' signifies the X-th dimension of the reduced embeddings).
\subsection{Training procedure}
For all our models we divided the data table into two parts: a training set and a validation set, using a random state of 42 for consistency. 
The training set contains $70\%$ of the original data, while the validation set holds the remaining $30\%$. 
This separation allows us to train our models on a subset of the data and then assess its performance on a different subset to ensure its generalization to new, unseen data.

\subsubsection{Random Forest}
%% TODO: Truc intro

For the Random Forest classifier we addressed the imbalance between the number of non-enzymes and enzymes, where non-enzymes outnumber enzymes approximately fourfold,
by duplicating the enzyme data in the training set four times.
This duplication ensures a more balanced dataset, allowing the model to be trained on an equal representation of both classes.

The classifier consists of a total of 200 decision trees.
Each tree has a maximum depth of 16, and a node is designated as a leaf only if it has a minimum of 8 samples.
The random state parameter is set to 42, ensuring reproducibility in the model's construction.
\subsubsection{Level 1 FNN}
\subsubsection{Level 2 FNN}


\subsection{Validation on test dataset}
%% TODO: Introduction of new.csv dataset

\subsubsection{Scoring metrics}

%% TODO: short sentence about scoring metrics → truc

%% TODO: Where to put scoring metrics?

\begin{align}
	&\textbf{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \\
    &\textbf{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} \\
    &\textbf{f1-score} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}} \\
	& \textbf{weighted f1-score} = \operatornamewithlimits{\sum}^{N}_{i=i} \frac{\text{Number of samples in class }i}{\text{Total number of samples}}  \times \text{f1-score}_{i}\nonumber\\
	&\quad \text{(where $N$ is the number of classes)} \\
    &\textbf{MCC score} = \frac{\text{TP} \cdot \text{TN} - \text{FP} \cdot \text{FN}}{\sqrt{(\text{TP} + \text{FP}) \cdot (\text{TP} + \text{FN}) \cdot (\text{TN} + \text{FP}) \cdot (\text{TN} + \text{FN})}}
\end{align}

The \textit{MCC} score serves as an indicator for the accuracy of binary classifications, with values ranging from -1 to 1. A score of 1 signifies a flawless prediction, 
-1 indicates a completely inaccurate prediction, and 0 suggests predictions at random.  

To assess the performance of our models, we emphasize the F1 score and MCC score. 
Given the imbalance in the test set, we also considered the weighted F1 score to ensure a more equitable evaluation of the model's 
effectiveness.

\end{methods}

\section{Results}	

\subsection{Random Forest Level 0}
Using the Random Forest model on the “new” test set, we achieved accurate predictions for about 
$90\%$ of positive cases (enzymes) and $98\%$ of negative cases (non-enzymes), even with an imbalanced test set comprising 
$392$ enzymes and 9876 non-enzymes.

%% Figure 1 of truc result
\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_1.jpeg}
\caption{Confusion Matrix of the Random Forest Model on the “new” dataset}\label{fig:04}
\end{figure}

Additionally, we attained an Accuracy of 97.6\%, a weighted f1-score of 97.8\%, and an MCC-score of 74.1\%. 
Figure \ref{fig:05} illustrates that our Random Forest model significantly outperformed the random baseline.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_3.jpeg}
\caption{Level 0 model comparison of Random Forest and baseline on “new” dataset}\label{fig:05}
\end{figure}

Furthermore, when we tested our model on a distinct test set, the "price" dataset,
it demonstrated a strong performance: only one enzyme was misclassified  (see figure \ref{fig:06}).
This reinforces the robustness and generalizability of our Random Forest model across diverse datasets.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_4.jpeg}
\caption{Confusion Matrix of the Random Forest Model on the “price” dataset}\label{fig:06}
\end{figure}


\subsubsection{Support Vector Machine}


\subsection{Level 1 performance}
\subsection{Level 2 performance}



%\centerline{\includegraphics{fig01.eps}}

%\begin{figure}[!tpb]%figure2
%%\centerline{\includegraphics{fig02.eps}}
%\caption{Caption, caption.}\label{fig:02}
%\end{figure}
\section{Discussion}

\section{Conclusion}

\section{Supplementary Information}

\subsection{K-nearest neighbors algorithm using ncd vectors}
A less popular approach of transforming string like input features into numerical values is the normalized compression distance (ncd) algorithm. 
The ncd algorithm is based on the idea that the similarity of two strings can be measured by the amount of information needed to describe one string given the other string. The ncd of two strings $x$ and $y$ is defined as follows:
\begin{equation}
	\text{ncd}(x,y) = \frac{C(xy)-\min(C(x),C(y))}{\max(C(x),C(y))}
\end{equation}
where $C(x)$ is the length of the compressed string $x$  and $C(xy)$ is the length of the concatenated string $xy$. 

We implemented this algorithm in python using \textit{gzip}, which is a loss less compression algorithm based on a combination of LZ77 and Huffman encoding. \cite{Rigler2007} 

The ncd algorithm was used to transform the amino acid sequences into numerical vectors by comparing each sequence to all other sequences in the training dataset.
This resulted in a $n$-dimensional numerical vector for each sequence, where $n$ is the amount of sequences in the training dataset where each
position in the vector represents the ncd of the sequence to the corresponding sequence in the training dataset.
These vectors were then used as input for the k-nearest neighbors algorithm.
Due to the exponential computational complexity of the ncd algorithm, we had to under sample the non enzyme dataset to match the amount of samples in our enzyme dataset, meaning
the positiv in the train dataset were balanced. 

When inferring unseen data, the ncd input vector was calculated by comparing it to all sequences in the training data set, thus also resulting in a $n$-dimensional numerical vector.
This means that the performance on new data is largely dependent on the training data set.

The performance of the k-nearest neighbors algorithm using ncd vectors compared to a random baseline is shown in figure \ref{fig:07}.
Although the mean F1 score of the k-nearest neighbors algorithm using ncd vectors lies at $0.728$, it did not perform better than the random baseline, which had a F1 score of $0.843$.
This indicates that the ncd approach has a worse precision and recall than the random baseline.
At the same time both classifiers have a low MCC 
score of $0.2$ and $0.01$ respectively, which indicates that both classifiers are not better than random guessing.

\begin{figure}[!tbp]
\includegraphics[width=0.5\textwidth]{assets/gzip_new.png}
\caption{Performance on test dataset compared to random baseline}\label{fig:07}
\end{figure}

The reason for the poor performance of the k-nearest neighbors algorithm using ncd vectors is
most likely due to the ncd algorithm not being suited for protein sequences as shown in \cite{GzipProteinCompression} as well as
the test dataset not being balanced, while the training dataset was.




\bibliographystyle{natbib} 
\bibliography{document}

\end{document}
