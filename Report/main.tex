\documentclass{bioinfo}
% Do not touch this
\copyrightyear{2015} \pubyear{2015}

% Do not touch this
\access{}
\appnotes{}

\begin{document}
\firstpage{1}

\subtitle{Enzyme Prediction}

\title[]{Enzyme Prediction Main Titel}
\author[]{Malte Alexander Weyrich, Thanh Truc Bui, Jan Philipp Hummel
and  Sofiia Kozoriz}

% Do not touch this
\address{}
\corresp{}
\history{}
\editor{}

% The command needs to be in the document, otherwise crash
\abstract{
\textbf{Motivation:} \\
\textbf{Results:}\\
\textbf{Availability: } https://github.com/github4touchdouble/nnet-enzyme.git \\
}

\maketitle

% TODO: Maybe replot the performance plots due tu the missplacement of the legend making it smaller 

\section{Abstract}
%  Malte →
The accurate prediction of enzyme commission numbers (EC numbers) is not only crucial for 
the classification and understanding of newly discovered enzymes but also for completing the annotation of already known enzymes.
Therefore, developing a reliable method for predicting EC numbers is very important.

%  Malte →
However, due to insufficient data, enzyme function prediction using machine learning remains challenging.
This paper proposes several methods for predicting enzymes in three different problem categories. Each category goes further into depth
of the enzyme classification system, starting with a binary classification (level 0) of enzymes and non-enzymes, followed by a multi-classification (level 1 \& 2) of enzymes into main classes (e.g., EC X.-.-.-) and subclasses (e.g., EC X.X.-.-).
Throughout developing our models, we used various input features and machine learning algorithms, of which the best will be thoroughly reviewed in this paper (see table \ref{tab:bestScores}).

%  Malte
\begin{table}[!htbp]
\processtable{A table showing the best-performing models per level \label{tab:bestScores}} {\begin{tabular}{@{}llll@{}}\toprule 
		Level &  Model & Weighted f1 score & MCC score\\\midrule
		0 & Random Forest & score & score\\
		1 &  Feedforward Neuronal Network & score & score \\
		2 &  Feedforward Neuronal Network & score & score\\\botrule
\end{tabular}}{}
\end{table}

\section{Introduction}

%  Truc
Enzymes play a vital role as biological catalysts, facilitating essential biochemical reactions in organisms.
Without enzyme catalysis, these reactions would either occur too slowly or be practically impossible.
Predominantly proteins, enzymes are designated specific functions; for instance, Oxidoreductases drive redox reactions, and Isomerases convert molecules into their isomers.
Therefore, a comprehensive understanding and a precise classification of enzymes are fundamental.

%  Truc
In the past, scientists attempted to categorize enzymes into groups and develop a logical rule set for naming.
However, the efforts were hindered by ambiguity.
A significant milestone occurred in 1956 with the establishment of an official international commission on enzyme classification (\cite{IUBMB}).
This marked the initiation of the contemporary enzyme classification system that forms the basis for our understanding of enzymes today.

%  Truc
In modern research, computational methods have become invaluable for enzyme classification; this shift has transformed the traditionally labor-intensive process.
Machine learning and data-driven models, in particular, have assumed a leading role, holding the potential for enhanced accuracy and efficiency in annotating enzymes within vast genomic data.

%  Truc
Several effective methods have been developed to address this challenge.
One notable example is DeepEC (\cite{DeepEC}), a deep learning-based computational framework designed for the precise and high-throughput prediction of EC numbers for protein sequences.
DeepEC employs three convolutional neural networks (CNNs) as a primary engine for EC number prediction and incorporates homology analysis for cases not classified by the CNNs.

%  Truc
Another noteworthy method is CLEAN (\cite{CleanArticle}), which stands for "contrastive learning–enabled 
enzyme annotation". CLEAN adopts a unique training objective, aiming to learn an embedding space for enzymes where the Euclidean distance 
reflects functional similarities. In CLEAN's task, sequences sharing the same EC number exhibit a small Euclidean distance, while sequences 
with different EC numbers have a larger distance. The model is trained using contrastive losses with supervision to achieve effective 
enzyme function prediction.

%  Truc & Malte 
In our research, we developed several classification models for each level, using a variety of machine-learning algorithms and input features.
The best-performing models per level will be thoroughly reviewed in this paper.
The first is a binary classifier determining whether a given protein functions as an enzyme. 
The second and third models are multi-classifiers, categorizing enzymes into classes and subclasses 
based on their specific functionalities. 
Our primary objective is to outperform each category's random baseline classifiers.

\begin{methods}
\section{Methods}
\subsection{Binary classification of enzymes}
% Jan
In the course of assigning EC numbers to protein sequences,
our initial step is to determine whether a given protein sequence is enzymatic or a non-enzymatic (level 0).
To identify the most effective approach for this task,
we conducted training the following binary classification models and compared their performance to select the optimal one:
\begin{enumerate}
	\item[(1)] k-Nearest-Neighbors
	\item[(2)] Random Forest 
	\item[(3)] Support Vector Machine
\end{enumerate}

The chosen method for binary classification will be detailed in the subsequent section, while section \ref{sec:unused binarys} will outline the methods that were not selected.

%  Truc
% Our initial goal is to categorize proteins as either enzymes or non-enzymes (level 0). 
% To accomplish this, we've opted for the Random Forest, a straightforward yet powerful machine learning method.
% The choice of Random Forest is driven by its effectiveness in handling classification tasks, making it a well-suited option for our specific protein classification objective.
% We built a Random Forest Classifier using the scikit-learn library with specific parameters, which will be explained in the training procedure. 

\subsection{Multi-classification of enzymes}
%  Malte →
We used a feedforward neural network (FNN) for levels 1 and 2 to classify the enzymes into their main classes and subclasses.
The FNN is an artificial neural network (ANN) that uses a series of layers to extract features from the input data and classify it into different categories,
thus making it a suitable choice for our multi-classification task. 

\subsection{Data preprocessing}

%  Malte & Truc
The protein data sets were taken from the UniProt database and were also used in the CLEAN publication on Enzyme function prediction (see \cite{CleanArticle}). 
The complete data pool of enzymes (called Split100 in \cite{CLEANgit}) contains 224.693 sequences. 
This pool was used to derive several smaller subsets (SplitX, $X \in \left\{10, 30, 50, 70\right\}$) where $X$ stands for the amount of similarity allowed between the sequences.
For reference, Split30 only contains sequences that share at most 30\% similarity with each other.
Since we want to reduce family overrepresentation we opted to use the Split30 dataset for our research,
containing 9.186 enzymes.
This redundancy reduction ensures that the model is not too biased towards a specific family of enzymes.
This subset was combined with a non-enzyme dataset holding 37.347 sequences, resulting in 46.533 sequences (see figure \ref{fig:DataPoolDist}).

%  TODO: introduce esm2 and ProtT5
For each sequence we inferred the esm2b embeddings (see \cite{ESM2}) and ProtT5 embeddings (see \cite{ProtT5}) 
using the esm2 model (see \cite{ESM2}) and the ProtT5 model (see \cite{ProtT5}) respectively.
These are large language models capable of encoding protein sequences into numerical vectors
by training on a large amount of protein sequences. 


Note that we additionally removed all sequences containing an ambiguous amino acid, such as 'U' or 'O', from the dataset, as well as
sequences with a length longer than 1022 amino acids. 
% TODO: Check reason fr length cutoff again <19.01.24, Malte Weyrich> %
This is due to the fact that the esm2 model used for the embeddings only accepts sequences 
with a maximum length of 1022 amino acids. 

% TODO: Maybe not include log scale
\begin{figure}[!htbp]
\includegraphics[width=0.5\textwidth]{./assets/l0_dist_all_train.png}
\caption{Barplot showing the amount of sequences per class in the complete data pool (logarithmic scale)}\label{fig:DataPoolDist}
\end{figure}

\subsubsection{Data preprocessing for level 0}
% Truc
As a random forest model relies on multiple decision trees, and each decision tree requires various features,
we opted to extract additional information from both the protein sequence and the esm2 embeddings \cite{ESM2}.

% Truc
From the amino acid mass table (see \cite{BioinformaticsSolutionsInc}) TODO: find author of mass table / another source
we computed the mass of protein sequences by adding up the individual masses of their amino acid components.
The esm2 embeddings, each represented by a $2560$-dimensional vector, underwent statistical analysis.
We computed the median, standard deviation, and vector magnitude by aggregating values across all $2560$ dimensions for each protein.
To simplify the embeddings, we applied Principal Component Analysis (\cite{scikit-learn})
separately to enzyme and non-enzyme datasets,
retaining $90\%$ of the variance. This process yielded reduced dimensions of $397$ for enzymes and $369$ for non-enzymes. 
Therefore, we reduced the dimensions to $397$, providing a streamlined representation of the protein embeddings while retaining crucial information for both 
enzyme (see figure \ref{fig:PCA_enzymes}) and non-enzyme (see figure \ref{fig:PCA_nonEnzymes}) datasets.

% TODO: Make the labels in the figures bigger
\begin{figure}[!bp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_1.jpeg}
\caption{The number of components needed to explain the variance in the enzyme dataset}\label{fig:PCA_enzymes}
\end{figure}

\begin{figure}[!tbp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_2.jpeg}
\caption{The number of components needed to explain the variance in the non enzyme dataset}\label{fig:PCA_nonEnzymes}
\end{figure}


% Truc
By combining the information from the proteins’ sequences, masses, and embeddings, we created a Pandas DataFrame that concatenates enzymes and non-enzymes,
including $401$ features: mass, embeddings median, embeddings standard deviation (std), embeddings magnitude, and dimension $1$ to $397$ of the reduced 
embeddings. 

% Truc
We also used the method SelectFromModel from the scikit-learn library (see \cite{scikit-learn})
to select features based on importance weights. 
The refined set of input features contains 
'Mass,' 'Emb median,' 'Emb std,' 'Emb magnitude,' and 'PCA 1' through 'PCA 47' (see table \ref{tab:ReducedInputRF}) excluding ‘PCA 29’, ‘PCA 31’, ‘PCA 35’, 
‘PCA 36’, ‘PCA 39’ to ‘PCA 42’, ‘PCA 44’ and ‘PCA 46’ (where 'Emb' represents embeddings and 'PCA X' signifies the X-th dimension of the reduced embeddings).

% Malte
\begin{table}[!htbp]
\setlength{\tabcolsep}{2pt}
\processtable{Input features for the Random Forest model \label{tab:ReducedInputRF}} {
	\begin{tabular}{@{}ccccccc@{}}
		\toprule 
		Mass & Emb median & Emb std. & Emb magnitude & PCA 1 & \dots & PCA 47\\
		\midrule
		60153,711 & -0,002189 & 0,227337 & 11,502516 & 0,450330 & \dots & -0,050497\\
		81547,542 & -0.002620 & 0,240143 & 12,150464 & -0,087801 & \dots & 0,220343 \\
		\dots & \dots & \dots & \dots & \dots & \dots & \dots \\
		\botrule
    \end{tabular}
}{}
\end{table}

\subsubsection{Data preprocessing for level 1}
% Malte →
For the level 1 prediction we labeled each sequence with its respective main class,
resulting in 7 classes (see figure \ref{fig:l1_dist_train}).
While the second main class is the most prevalent, 
the seventh main class is the least prevalent with only
containing 190 representatives.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{./assets/l1_dist_enzymes.png}
\caption{Main distribution in Split30 (logarithmic scale)}\label{fig:l1_dist_train}
\end{figure}


\subsubsection{Data preprocessing for level 2}
% Malte
For the level 2 prediction we labeled each sequence with its respective subclass (see figure \ref{fig:l2_dist_train}).

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{./assets/l2_dist_all_train_ver.png}
\caption{Subclass distribution in Split30 (logarithmic scale)}\label{fig:l2_dist_train}
\end{figure}

The problem of imbalanced data and underrepresentation of some labels is even more pronounced when labeling the sequences with their respective subclass.
Some enzyme families had less than 10 representatives in the dataset, making it difficult for the model to learn the patterns of these families.
To address issue, we combined underrepresented families of the same main class into a single class 
(e.g. \textbf{EC 2.2.-.-}, \textbf{EC 2.9.-.-} and \textbf{EC 2.19.-.-} were merged into the label \textbf{EC 2.2|9|19.-.-}).
This way we guaranteed that each subclass had at least 10 representatives in the dataset.
This procedure had to be applied to each main class separately, as the subclasses are only unique within their respective main class.

\subsection{Training procedure}
% Truc
For all our models we divided the data table into two parts: a training set and a validation set, using a random state of 42 for consistency. 
The training set contains $70\%$ of the original data, while the validation set holds the remaining $30\%$. 
This separation allows us to train our models on a subset of the data and then assess its performance on a different subset to ensure its generalization to new, unseen data.

\subsubsection{Level 0 - Random Forest}
% Truc
A straightforward yet powerful machine learning method is the Random Forest algorithm.
The choice of Random Forest is driven by its effectiveness in handling classification tasks, making it a well-suited option for our specific protein classification objective.
We built a Random Forest Classifier using the scikit-learn library with specific parameters, which will be explained in the training procedure. 
For the Random Forest classifier we addressed the imbalance between the number of non-enzymes and enzymes, where non-enzymes outnumber enzymes approximately fourfold,
by duplicating the enzyme data in the training set four times.
This duplication ensures a more balanced dataset, allowing the model to be trained on an equal representation of both classes.

% Truc
The classifier consists of a total of 200 decision trees.
Each tree has a maximum depth of 16, and a node is designated as a leaf only if it has a minimum of 8 samples.
The random state parameter is set to 42, ensuring reproducibility in the model's construction.
\subsubsection{Level 1 - FNN}
% Kida
\subsubsection{Level 2 - FNN}
% Malte
For the level 2 prediction we used a feedforward neural network (FNN) to classify the enzymes into their respective subclasses.
The network was implemented using Tensorflow, which is an open-source library for machine learning (see \cite{tensorflow2015-whitepaper}).
The architecture consists of one input layer, two hidden layers and one output layer.
In between the hidden layers we used dropout layers to prevent overfitting.
The layers are fully connected and the activation function used is the rectified linear unit (ReLU).
The amount of perceptrons used per layer and the dropout rate were optimized using the optuna library (see \cite{optuna_2019}).
This resulted in 262 perceptrons in the first hidden layer and 121 perceptrons in the second hidden layer.
The dropout rate was set to 0.23.
While the input layer had the same dimension as the esm2b embeddings (2560), the output layer had a dimension of 51,
since there are 51 subclasses in the training dataset (see figure \ref{fig:l2_dist_train}).
To further address the imbalance in the dataset, we used class weights to penalize the model more for misclassifying the minority classes
while being more lenient towards misclassifying the majority classes.
During training we used early stopping with a patience of 15 epochs to prevent overfitting.


\subsection{Validation on test dataset}
%  TODO: Introduction of new.csv dataset + us combining it with the non-enzymes test dataset

\subsubsection{Scoring metrics}

% Truc
To assess the performance of our models, we emphasize the F1 score and MCC score. 
Given the imbalance in the test set, we also considered the weighted F1 score for the multiclassification models
to ensure a more equitable evaluation of the model's effectiveness.

The MCC score serves as an indicator for the accuracy of binary classifications, with values ranging from -1 to 1. 
A score of 1 signifies a flawless prediction, -1 indicates a completely inaccurate prediction, and 0 suggests predictions at random.
This scoring metric takes into account the entire confusion matrix, making it especially
useful for the level 1 and 2 models, which are multiclassification models.

% TODO: F1 Score needs to be introduced <19.01.24, Malte Weyrich> %

\begin{align}
	&\textbf{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \\
    &\textbf{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} \\
    &\textbf{f1-score} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}} \\
	% & \textbf{weighted f1-score} = \operatornamewithlimits{\sum}^{N}_{i=i} \frac{\text{Number of samples in class }i}{\text{Total number of samples}}  \times \text{f1-score}_{i}\nonumber\\
	% &\quad \text{(where $N$ is the number of classes)} \\
    &\textbf{MCC score} = \frac{\text{TP} \cdot \text{TN} - \text{FP} \cdot \text{FN}}{\sqrt{(\text{TP} + \text{FP}) \cdot (\text{TP} + \text{FN}) \cdot (\text{TN} + \text{FP}) \cdot (\text{TN} + \text{FN})}}
\end{align}




\subsubsection{Bootstarpping procedure}
Bootstrapping describes randomly drawing $n$ samples with replacement from the same data pool $B$ times.
We used this method to further evaluate our model scores, as it allows us to estimate the mean, standard error, and confidence interval of the scores.
We chose $B$ to be 10 000 and $n$ to be the predicted samples.
This means that we drew 10 000 pairs of true labels and predicted labels out of the test set with replacement,
allowing us to calculate the scores for each bootstrapped dataset and then estimate the performance's mean, 
standard error, and confidence interval.
Each score has been rounded to significant digits using the standard error.
We calculated the error bars using an $\alpha$ of 0.05 and the standard error of the scores.
The performance plots show the mean score and the confidence intervals as error bars.

\end{methods}

\section{Results}	

\subsection{Random Forest Level 0}\label{sec:RF_level0}
Using the Random Forest model on the “new” test set, we achieved accurate predictions for about 
$90\%$ of positive cases (enzymes) and $98\%$ of negative cases (non-enzymes), even with an imbalanced test set comprising 
$392$ enzymes and 9876 non-enzymes. As a reference we provided the confusion matrix in figure \ref{fig:RF_conf_l0}.

%  Figure 1 of truc result
\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_1.jpeg}
\caption{Confusion Matrix of the Random Forest Model on the “new” dataset}\label{fig:RF_conf_l0}
\end{figure}

Additionally, we attained an Accuracy of 97.6\%, a weighted f1-score of 97.8\%, and an MCC-score of 74.1\%. 
Figure \ref{fig:RF_scores_l0} illustrates that our Random Forest model significantly outperformed the random baseline.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_3.jpeg}
\caption{Level 0 model comparison of Random Forest and baseline on “new” dataset}\label{fig:RF_scores_l0}
\end{figure}

Furthermore, when we tested our model on a distinct test set, the "price" dataset,
it demonstrated a strong performance: only one enzyme was misclassified  (see figure \ref{fig:RF_conf_l0_price}).
This reinforces the robustness and generalizability of our Random Forest model across diverse datasets.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_4.jpeg}
\caption{Confusion Matrix of the Random Forest Model on the “price” dataset}\label{fig:RF_conf_l0_price}
\end{figure}


\subsection{Level 1 performance}

% \begin{figure}[!htpb]
% \includegraphics[width=0.5\textwidth]{assets/l1_scores.png}
% \caption{}\label{fig:FNN_scores_l1}
% \end{figure}
%
% \begin{figure}[!htpb]
% \includegraphics[width=0.5\textwidth]{assets/l1_conf_new.png}
% \caption{}\label{fig:FNN_conf_l1}
% \end{figure}
% \subsection{Level 2 performance}
%
% \begin{figure}[!htpb]
% \includegraphics[width=0.5\textwidth]{assets/l2_scores.png}
% \caption{}\label{fig:FNN_scores_l2}
% \end{figure}
\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/l2_conf_test_new_all.png}
\caption{Normalized confusion matrix of level 2 prediction. 
The test dataset only had a subset (28 subclasses) of the subclasses present in the training dataset (51 subclasses), 
which is indicated by the voids in the matrix. 
The reasonably low noise per row indicates that the model can distinguish between the present subclasses of the new dataset.}\label{fig:FNN_conf_l2}
\end{figure}

\section{Discussion}

\section{Conclusion}

\section{Supplementary Information}

\subsection{Additional binary classification models}\label{sec:unused binarys}
% Jan
As we thoroughly reviewed the Random Forest model in section \ref{sec:RF_level0}, we will now outline the other binary classification models we considered for this task.

\subsubsection{k-Nearest-Neighbors (kNN)}
% Jan
kNN is a non-parametric classification method,
which assigns a new object to the most common class amongst the most similar k objects in the data set (\cite{knn_principles}). We implemented three kNN models in Python using scikit-learns Nearest Neighbors library and 
these models using three distinct types of input features:
\begin{enumerate}
	\item[(1)] Protein embedding vectors encoded by ProtT5 (see \cite{ProtT5})
    \item[(2)] Protein embedding vectors encoded by ESM2 (see \cite{ESM2})
    \item[(3)] Normalized compression distance vectors (see \cite{GzipTextClassification})
\end{enumerate}


\paragraph{(1) kNN using ProtT5 encoded embedding vectors:}

% Jan
Each protein sequence, represented as a numerical vector of 1024 dimensions, is placed as a data 
point within a 1024-dimensional space. The kNN classifier, using k=7,
categorizes each data point to the class that is most common among its k nearest neighbors in the 1024-dimensional space.
Figure 
% TODO: insert Jans fig \ref{fig:03} 
illustrates the performance comparison between the kNN algorithm using ProtT5 encoded embedding vectors versus a random baseline. 
We observe that the mean F1 score for this model demonstrates a slight improvement over the random baseline,
recording 0. 857 compared to 0.843. This implies that this approach showcases superior precision and recall in contrast to the random baseline.
This models Mathews Correlation Coefficient of 0.728 indicates that this approach preforms significantly better than random guessing.


\paragraph{(2) kNN using ESM2 encoded embedding vectors:}
% Jan
Each protein sequence, represented as a numerical vector of 2560 dimensions, is placed as a data point within a 2560-dimensional space.
The kNN classifier, using k=7, categorizes each data point to the class that is most common among its k nearest neighbors in the 2560-dimensional space.
Figure 
% TODO: insert \ref{fig:03} 
illustrates the performance comparison between the kNN algorithm using ESM2 encoded embedding vectors versus a random baseline.
Unlike above, we observe that the mean F1 score for this model demonstrates a slight regression over the random baseline, recording 0.833
compared to 0.843. This implies that this approach showcases inferior precision and recall in contrast to the random baseline.
Despite being lower than the previous case, this models Mathews Correlation Coefficient of 0.689 still indicates that this approach preforms 
better than random guessing.

\paragraph{(3) kNN using normalized compression distance vectors:}
The normalized compression distance ($ncd$) algorithm transforms string like input features into numerical values and is based on the concept of measuring 
the similarity of two strings by the amount of information needed to describe the one string given the other string. 
Given two strings $x$ and $y$, the $ncd$ is defined as follows:

\begin{equation}
	ncd(x,y) = \frac{C(xy)-\min(C(x),C(y))}{\max(C(x),C(y))}
\end{equation}
where $C(x)$ is the length of the compressed string $x$, $C(xy)$ is the length of the concatenated string $xy$. 

We implemented this algorithm in python using \textit{gzip}, which is a loss less compression algorithm based on a combination of LZ77 and Huffman encoding (\cite{Rigler2007}).
The $ncd$ algorithm transformed amino acid sequences into numerical vectors by comparing each sequence with all others in the training dataset.
This transformation yielded an $n$-dimensional numerical vector for each sequence, where $n$ represents the number of sequences in the training dataset.
Each position in the input vector signifies the $ncd$ of the sequence concerning the corresponding sequence in the training dataset. 
These vectors were then used as input for the k-nearest neighbors algorithm.
Due to the exponential computational complexity of the $ncd$ algorithm, we used under sampling of the non-enzyme dataset to match the sample size of our enzyme dataset,
ensuring balance in the positive instances within the training dataset. 

When inferring unseen data, the $ncd$ input vector was calculated by comparing it to all sequences in 
the training data set, resulting in a $n$-dimensional numerical vector.
Consequently, the performance on new data heavily relies on the characteristics of the training dataset.

The performance comparison between the kNN algorithm using $ncd$ vectors versus a random baseline are illustrated in figure \ref{fig:l0_comp_all}
which shows the performance of all level 0 models.
Despite the mean F1 score of the kNN lying at $0.728$, it did not perform better than the random baseline, which 
achieved a F1 score of $0.843$.
This observation suggests that the ncd approach exhibits inferior precision and recall compared to the random baseline. 
Additionally, both classifiers exhibit a low MCC score of 0.2 and 0.01, respectively, indicating that neither classifier performs better than random guessing.

The reason for the poor performance of the k-nearest neighbors algorithm using $ncd$ vectors is
most likely due to the ncd algorithm not being suited for protein sequences as shown in \cite{GzipProteinCompression} as well as
the test dataset not being balanced, while the training dataset was.

\subsubsection{Support Vector Machine (SVM)}
% Jan
SVM is an algorithm that assigns labels to objects by learning from examples (\cite{svm}).
We implemented three SVM models in Python using scikit-learns C-Support Vector classification library and these models using three distinct types of 
input features: 
\begin{enumerate}
	\item[(1)] Protein embedding vectors encoded by ProtT5 (\cite{ProtT5})
	\item[(2)] Protein embedding vectors encoded by ESM2 (\cite{ESM2})
	\item[(3)] One hot encoded protein sequence vectors (\cite{oh_encoding})
\end{enumerate}

%

\paragraph{(1) SVM using ProtT5 encoded embedding vectors:}

\paragraph{(2) SVM using ESM2 encoded embedding vectors:}

\paragraph{(3) SVM using one hot encoded protein sequence vectors:}


\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.5\textwidth]{./assets/l0_combined_scores.png}
	\caption{Comparison of all level 0 models}
	\label{fig:l0_comp_all}
\end{figure}


\bibliographystyle{natbib} 
\bibliography{document}

\end{document}
