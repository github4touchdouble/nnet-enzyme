\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}

\begin{document}
\firstpage{1}

\subtitle{Subject Section}

\title[References]{This is a title}
%  NOTE: I deleted the content of 
\author[]{Corresponding Author\,$^{\text{\sfb 1,}*}$, Co-Author\,$^{\text{\sfb 2}}$ and Co-Author\,$^{\text{\sfb 2,}*}$}
\address{$^{\text{\sf 1}}$Department, Institution, City, Post Code, Country and \\
$^{\text{\sf 2}}$Department, Institution, City, Post Code,
Country.}

\corresp{$^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} \\
	\textbf{Results:}\\
\textbf{Availability:} \\
\textbf{Contact:} \href{name@bio.com}{name@bio.com}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle

\section{Abstract}
%  Malte
The accurate prediction of enzyme commission numbers (EC numbers) is not only crucial for 
the classification and understanding of newly discovered enzymes but also for completing the annotation of already known enzymes.
Therefore, developing a reliable method for predicting EC numbers is of great importance.

However, due to insufficient data, enzyme function prediction using machine learning remains challenging.
In this paper, we propose several methods for predicting enzymes in three different problem categories. Each category goes further into depth
of the enzyme classification system, starting with a binary classification (level 0) of enzymes and non-enzymes, followed by a multi-classification (level 1 \& 2) of enzymes into main classes (e.g. EC X.-.-.-) and subclasses (e.g EC X.X.-.-).
Throughout the developing of our models, we used a variety of different input features and machine learning algorithms, of which the best will be thoroughly reviewed in this paper (see table \ref{tab:bestScores}).

\begin{table}[!htbp]
\processtable{A table showing the best-performing models per level \label{tab:bestScores}} {\begin{tabular}{@{}llll@{}}\toprule 
		Level &  Model & Weighted f1 score & MCC score\\\midrule
		0 & Random Forest & score & score\\
		1 &  Feedforward Neuronal Network & score & score \\
		2 &  Feedforward Neuronal Network & score & score\\\botrule
\end{tabular}}{}
\end{table}

\section{Introduction}

%  Truc
Enzymes play a vital role as biological catalysts, facilitating essential biochemical reactions in organisms.
Without enzyme catalysis, these reactions would either occur too slowly or be practically impossible.
Predominantly proteins, enzymes are designated specific functions; for instance, Oxidoreductases drive redox reactions, and Isomerases convert molecules into their isomers.
Therefore, a comprehensive understanding and a precise classification of enzymes are fundamental.

In the past, scientists attempted to categorize enzymes into groups and develop a logical rule set for naming.
However, the efforts were hindered by ambiguity.
A significant milestone occurred in 1956 with the establishment of an official international commission on enzyme classification (\cite{IUBMB}).
This marked the initiation of the contemporary enzyme classification system that forms the basis for our understanding of enzymes today.

In modern research, computational methods have become invaluable for enzyme classification; this shift has transformed the traditionally labor-intensive process.
Machine learning and data-driven models, in particular, have assumed a leading role, holding the potential for enhanced accuracy and efficiency in annotating enzymes within vast genomic data.

Several effective methods have been developed to address this challenge.
One notable example is DeepEC (\cite{DeepEC}), a deep learning-based computational framework designed for the precise and high-throughput prediction of EC numbers for protein sequences.
DeepEC employs three convolutional neural networks (CNNs) as a primary engine for EC number prediction and incorporates homology analysis for cases not classified by the CNNs.

Another noteworthy method is CLEAN (\cite{CleanArticle}), which stands for "contrastive learning–enabled 
enzyme annotation". CLEAN adopts a unique training objective, aiming to learn an embedding space for enzymes where the Euclidean distance 
reflects functional similarities. In CLEAN's task, sequences sharing the same EC number exhibit a small Euclidean distance, while sequences 
with different EC numbers have a larger distance. The model is trained using contrastive losses with supervision to achieve effective 
enzyme function prediction.

%  Truc & Malte
In our research, we developed several classification models for each level, using a variety of machine learning algorithms and input features.
The best performing models per level will be thoroughly reviewed in this paper.
The first is a binary classifier that determines whether a given protein functions as an enzyme. 
The second and third models are  multi-classifiers, tasked with categorizing enzymes into different classes and subclasses 
based on their specific functionalities. 
Our primary objective is to remarkably outperform the random baseline classifiers in each category.

\begin{methods}
\section{Methods}
\subsection{Binary classification of enzymes}
In the course of assigning EC numbers to protein sequences,
our initial step is to determine whether a given protein sequence is enzymatic or a non-enzymatic (level 0).
To identify the most effective approach for this task,
we conducted training the following binary classification models and compared their performance to select the optimal one:
\begin{enumerate}
	\item[(1)] k-Nearest-Neighbors
	\item[(2)] Random Forest 
	\item[(3)] Support Vector Machine
\end{enumerate}

The chosen methods will be detailed in the subsequent section, while Section 7.1 will outline the methods that were not selected.

%  Truc
% Our initial goal is to categorize proteins as either enzymes or non-enzymes (level 0). 
% To accomplish this, we've opted for the Random Forest, a straightforward yet powerful machine learning method.
% The choice of Random Forest is driven by its effectiveness in handling classification tasks, making it a well-suited option for our specific protein classification objective.
% We built a Random Forest Classifier using the scikit-learn library with specific parameters, which will be explained in the training procedure. 

%  Malte
\subsection{Multi-classification of enzymes}
For the second and third level, we used a feedforward neural network (FNN) to classify the enzymes into their respective main classes and subclasses.
The FNN is a type of artificial neural network (ANN) that uses a series of layers to extract features from the input data and classify it into different categories,
thus making it a suitable choice for our multi-classification task. The library used to build the FNN is Tensorflow, which is an open-source library for machine learning.

\subsection{Data preprocessing in general}

%  Malte & Truc
The protein data sets were taken from the UniProt database and were also used in the CLEAN publication on Enzyme function prediction (see \cite{CleanArticle}). 
The complete data pool of enzymes (called Split100 in \cite{CLEANgit}) contains 224.693 sequences. 
This pool was used to derive several smaller subsets (SplitX, $X \in \left\{10, 30, 50, 70\right\}$) where $X$ stands for the amount of similarity allowed between the sequences.
For reference, Split30 only contains sequences that share at most 30\% similarity with each other.
Since we want to avoid homology-based inference as much as possible while 
maintaining a reasonable amount of data to train our models on, we opted to use the Split30 dataset for our research,
containing 9.186 enzymes. This subset was combined with a non-enzyme dataset holding 37.347 sequences, resulting in 46.533 sequences (see figure \ref{fig:DataPoolDist}).
Note that we additionally removed all sequences containing an ambiguous amino acid, such as 'U' or 'O', from the dataset, as well as
sequences with a length longer than 1022 amino acids. 
This is due to the fact that the esm2b model used for the embeddings only accepts sequences 
with a maximum length of 1022 amino acids. %  TODO: add reference to esm2b model und mit Jan abklären

\begin{figure}[!htbp]
\includegraphics[width=0.5\textwidth]{assets/data_pool_complete.png}
\caption{Barplot showing the amount of sequences per class in the complete data pool}\label{fig:DataPoolDist}
\end{figure}

\subsection{Data preprocessing for level 0}
As a random forest model relies on multiple decision trees, and each decision tree requires various features,
we opted to extract additional information from both the protein sequence and the esm2 embeddings. % FIX: embeddings

From the amino acid mass table (see \cite{BioinformaticsSolutionsInc}) we computed the mass of protein sequences by adding up the individual masses of their amino acid components.
The esm2 embeddings, each represented by a $2560$-dimensional vector, underwent statistical analysis.
We computed the median, standard deviation, and vector magnitude by aggregating values across all $2560$ dimensions for each protein.
To simplify the embeddings, we applied Principal Component Analysis (\cite{scikit-learn})
separately to enzyme and non-enzyme datasets,
retaining $90\%$ of the variance. This process yielded reduced dimensions of $397$ for enzymes and $369$ for non-enzymes. 
Therefore, we reduced the dimensions to $397$, providing a streamlined representation of the protein embeddings while retaining crucial information for both 
enzyme (see figure \ref{fig:PCA_enzymes}) and non-enzyme (see figure \ref{fig:PCA_nonEnzymes}) datasets.

\begin{figure}[!bp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_1.jpeg}
\caption{The number of components needed to explain the variance in the enzyme dataset}\label{fig:PCA_enzymes}
\end{figure}

\begin{figure}[!tbp]
\includegraphics[width=0.5\textwidth]{assets/Data_Truc_2.jpeg}
\caption{The number of components needed to explain the variance in the non enzyme dataset}\label{fig:PCA_nonEnzymes}
\end{figure}


By combining the information from the proteins’ sequences, masses, and embeddings, we created a Pandas DataFrame that concatenates enzymes and non-enzymes,
including $401$ features: mass, embeddings median, embeddings standard deviation (std), embeddings magnitude, and dimension $1$ to $397$ of the reduced 
embeddings. 

We also used the method SelectFromModel from the scikit-learn library (see \cite{scikit-learn})
to select features based on importance weights. 
The refined set of input features contains 
'Mass,' 'Emb median,' 'Emb std,' 'Emb magnitude,' and 'PCA 1' through 'PCA 47' (see table \ref{tab:ReducedInputRF}) excluding ‘PCA 29’, ‘PCA 31’, ‘PCA 35’, 
‘PCA 36’, ‘PCA 39’ to ‘PCA 42’, ‘PCA 44’ and ‘PCA 46’ (where 'Emb' represents embeddings and 'PCA X' signifies the X-th dimension of the reduced embeddings).

\begin{table}[!htbp]
\setlength{\tabcolsep}{2pt}
\processtable{Input features for the Random Forest model \label{tab:ReducedInputRF}} {
	\begin{tabular}{@{}ccccccc@{}}
		\toprule 
		Mass & Emb median & Emb std. & Emb magnitude & PCA 1 & \dots & PCA 47\\
		\midrule
		60153,711 & -0,002189 & 0,227337 & 11,502516 & 0,450330 & \dots & -0,050497\\
		81547,542 & -0.002620 & 0,240143 & 12,150464 & -0,087801 & \dots & 0,220343 \\
		\dots & \dots & \dots & \dots & \dots & \dots & \dots \\
		\botrule
    \end{tabular}
}{}
\end{table}

\subsection{Training procedure}
For all our models we divided the data table into two parts: a training set and a validation set, using a random state of 42 for consistency. 
The training set contains $70\%$ of the original data, while the validation set holds the remaining $30\%$. 
This separation allows us to train our models on a subset of the data and then assess its performance on a different subset to ensure its generalization to new, unseen data.

\subsubsection{Random Forest}
%  TODO: Truc intro

A straightforward yet powerful machine learning method is the Random Forest algorithm.
The choice of Random Forest is driven by its effectiveness in handling classification tasks, making it a well-suited option for our specific protein classification objective.
We built a Random Forest Classifier using the scikit-learn library with specific parameters, which will be explained in the training procedure. 
For the Random Forest classifier we addressed the imbalance between the number of non-enzymes and enzymes, where non-enzymes outnumber enzymes approximately fourfold,
by duplicating the enzyme data in the training set four times.
This duplication ensures a more balanced dataset, allowing the model to be trained on an equal representation of both classes.

The classifier consists of a total of 200 decision trees.
Each tree has a maximum depth of 16, and a node is designated as a leaf only if it has a minimum of 8 samples.
The random state parameter is set to 42, ensuring reproducibility in the model's construction.
\subsubsection{Level 1 FNN}
\subsubsection{Level 2 FNN}


\subsection{Validation on test dataset}
%  TODO: Introduction of new.csv dataset

\subsubsection{Scoring metrics}

%  TODO: short sentence about scoring metrics → truc

%  TODO: Where to put scoring metrics?

\begin{align}
	&\textbf{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \\
    &\textbf{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} \\
    &\textbf{f1-score} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}} \\
	& \textbf{weighted f1-score} = \operatornamewithlimits{\sum}^{N}_{i=i} \frac{\text{Number of samples in class }i}{\text{Total number of samples}}  \times \text{f1-score}_{i}\nonumber\\
	&\quad \text{(where $N$ is the number of classes)} \\
    &\textbf{MCC score} = \frac{\text{TP} \cdot \text{TN} - \text{FP} \cdot \text{FN}}{\sqrt{(\text{TP} + \text{FP}) \cdot (\text{TP} + \text{FN}) \cdot (\text{TN} + \text{FP}) \cdot (\text{TN} + \text{FN})}}
\end{align}

The \textit{MCC} score serves as an indicator for the accuracy of binary classifications, with values ranging from -1 to 1. A score of 1 signifies a flawless prediction, 
-1 indicates a completely inaccurate prediction, and 0 suggests predictions at random.  

To assess the performance of our models, we emphasize the F1 score and MCC score. 
Given the imbalance in the test set, we also considered the weighted F1 score to ensure a more equitable evaluation of the model's 
effectiveness.

\end{methods}

\section{Results}	

\subsection{Random Forest Level 0}
Using the Random Forest model on the “new” test set, we achieved accurate predictions for about 
$90\%$ of positive cases (enzymes) and $98\%$ of negative cases (non-enzymes), even with an imbalanced test set comprising 
$392$ enzymes and 9876 non-enzymes. As a reference we provided the confusion matrix in figure \ref{fig:RF_conf_l0}.

%  Figure 1 of truc result
\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_1.jpeg}
\caption{Confusion Matrix of the Random Forest Model on the “new” dataset}\label{fig:RF_conf_l0}
\end{figure}

Additionally, we attained an Accuracy of 97.6\%, a weighted f1-score of 97.8\%, and an MCC-score of 74.1\%. 
Figure \ref{fig:RF_scores_l0} illustrates that our Random Forest model significantly outperformed the random baseline.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_3.jpeg}
\caption{Level 0 model comparison of Random Forest and baseline on “new” dataset}\label{fig:RF_scores_l0}
\end{figure}

Furthermore, when we tested our model on a distinct test set, the "price" dataset,
it demonstrated a strong performance: only one enzyme was misclassified  (see figure \ref{fig:RF_conf_l0_price}).
This reinforces the robustness and generalizability of our Random Forest model across diverse datasets.

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/Results_Truc_4.jpeg}
\caption{Confusion Matrix of the Random Forest Model on the “price” dataset}\label{fig:RF_conf_l0_price}
\end{figure}


\subsubsection{Support Vector Machine}


\subsection{Level 1 performance}

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/l1_scores.png}
\caption{}\label{fig:FNN_scores_l1}
\end{figure}

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/l1_conf_new.png}
\caption{}\label{fig:FNN_conf_l1}
\end{figure}
\subsection{Level 2 performance}

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/l2_scores.png}
\caption{}\label{fig:FNN_scores_l2}
\end{figure}

\begin{figure}[!htpb]
\includegraphics[width=0.5\textwidth]{assets/l2_conf_test_new.png}
\caption{}\label{fig:FNN_conf_l2}
\end{figure}

\section{Discussion}

\section{Conclusion}

\section{Supplementary Information}

\subsection{K-nearest neighbors algorithm using ncd vectors}
A less popular approach of transforming string like input features into numerical values is the normalized compression distance (ncd) algorithm. 
The ncd algorithm is based on the idea that the similarity of two strings can be measured by the amount of information needed to describe one string given the other string. The ncd of two strings $x$ and $y$ is defined as follows:
\begin{equation}
	\text{ncd}(x,y) = \frac{C(xy)-\min(C(x),C(y))}{\max(C(x),C(y))}
\end{equation}
where $C(x)$ is the length of the compressed string $x$  and $C(xy)$ is the length of the concatenated string $xy$. 

We implemented this algorithm in python using \textit{gzip}, which is a loss less compression algorithm based on a combination of LZ77 and Huffman encoding. \cite{Rigler2007} 

The ncd algorithm was used to transform the amino acid sequences into numerical vectors by comparing each sequence to all other sequences in the training dataset.
This resulted in a $n$-dimensional numerical vector for each sequence, where $n$ is the amount of sequences in the training dataset where each
position in the vector represents the ncd of the sequence to the corresponding sequence in the training dataset.
These vectors were then used as input for the k-nearest neighbors algorithm.
Due to the exponential computational complexity of the ncd algorithm, we had to under sample the non enzyme dataset to match the amount of samples in our enzyme dataset, meaning
the positiv in the train dataset were balanced. 

When inferring unseen data, the ncd input vector was calculated by comparing it to all sequences in the training data set, thus also resulting in a $n$-dimensional numerical vector.
This means that the performance on new data is largely dependent on the training data set.

The performance of the k-nearest neighbors algorithm using ncd vectors compared to a random baseline is shown in figure \ref{fig:gzip_scores}.
Although the mean F1 score of the k-nearest neighbors algorithm using ncd vectors lies at $0.728$, it did not perform better than the random baseline, which had a F1 score of $0.843$.
This indicates that the ncd approach has a worse precision and recall than the random baseline.
At the same time both classifiers have a low MCC 
score of $0.2$ and $0.01$ respectively, which indicates that both classifiers are not better than random guessing.

\begin{figure}[!htbp]
\includegraphics[width=0.5\textwidth]{assets/gzip_new.png}
\caption{Performance on test dataset compared to random baseline}\label{fig:gzip_scores}
\end{figure}

The reason for the poor performance of the k-nearest neighbors algorithm using ncd vectors is
most likely due to the ncd algorithm not being suited for protein sequences as shown in \cite{GzipProteinCompression} as well as
the test dataset not being balanced, while the training dataset was.




\bibliographystyle{natbib} 
\bibliography{document}

\end{document}
