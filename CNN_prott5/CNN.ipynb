{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc66e30-70a8-470c-bcda-f820be37013e",
   "metadata": {},
   "source": [
    "# Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9079ec71-6002-42d5-85e6-c0fb3da9fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c47f51-0eff-4e83-8a38-e7b5da8efd5d",
   "metadata": {},
   "source": [
    "# Hyperparameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92aa1b6-1be8-469e-a4fa-48a0167f91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = {\n",
    "    'epoch_num': 200,     # Number of times that the model will see the entire dataset during training \n",
    "    'lr': 5e-5,           # Learning rate: controls the size of the step the optimizer takes towards the local minimum during training.\n",
    "    'weight_decay': 5e-4, # L2 penalty: helps prevent overfitting by adding a penalty to the value of the model's weights\n",
    "    'num_workers': 3, # Número de threads do dataloader.\n",
    "    'num_classes' : 10, \n",
    "    'batch_size': 20,     # Tamanho do batch.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141fbbd-317a-40e3-9e2a-21902c28b73a",
   "metadata": {},
   "source": [
    "# Definition of the default hardware used (must be GPU??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ce82ef-dc9f-4bea-ba79-3209deba5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db09890-b914-4a60-81ae-bea181161ccd",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1548d76-41c0-4d29-a46b-f0f40bd2d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 9204\n",
      "Embedding 1: A0A024RBG1\n",
      "Embedding 2: A0A024SMV2\n",
      "Embedding 3: A0A060S684\n",
      "Embedding 4: A0A075TXZ3\n",
      "Embedding 5: A0A077K8G3\n",
      "Embedding 6: A0A077YBL0\n",
      "Embedding 7: A0A096ZEC9\n",
      "Embedding 8: A0A096ZED0\n",
      "Embedding 9: A0A0A1GNF2\n",
      "Embedding 10: A0A0B0QJN8\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file for reading\n",
    "with h5py.File('split30_prott5.h5', 'r') as f:\n",
    "\n",
    "# Load the embeddings (keys in the HDF5 file) dataset into a variable\n",
    "    embeddings = list(f.keys())\n",
    "    #print(embeddings)\n",
    "    \n",
    "# Get the length of the embeddings (number of items in the list)\n",
    "    embeddings_length = len(embeddings)\n",
    "    print(\"Number of embeddings:\", embeddings_length)\n",
    "     \n",
    "#Print the first 10 embeddings\n",
    "    for i in range(min(10, embeddings_length)):\n",
    "        print(f\"Embedding {i+1}:\", embeddings[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555934b-6408-4924-990a-e02725e7d662",
   "metadata": {},
   "source": [
    "# Train and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7bb20b-a878-4431-8355-2c93c6da5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(embeddings)).tolist() #list called indice with random separation of the embeddings \n",
    "\n",
    "train_size = int(0.8*len(embeddings)) #training dataset will be 80% \n",
    "\n",
    "embeddings_train = [embeddings[i] for i in indices[:train_size]]  # Select the first 80% of embeddings\n",
    "embeddings_test = [embeddings[i] for i in indices[train_size:]]   # Select the last 20% of embeddings\n",
    "\n",
    "#print(len(embeddings_train), len(embeddings_test))\n",
    "\n",
    "# Save the training and test datasets in separate variables (lists)\n",
    "train_data = embeddings_train\n",
    "test_data = embeddings_test\n",
    "\n",
    "#print(embeddings_train)\n",
    "#print(embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901903e-9d37-4734-b84b-c907f1a08076",
   "metadata": {},
   "source": [
    "# Transform list train/data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30039ed-0db4-4346-a4b1-867517d9e4d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Transform the list into a tensor with float data type\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tensor_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tensor_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor_train)\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "#Transform the list into a tensor with float data type\n",
    "tensor_train = torch.tensor(train_data, dtype=torch.float32)\n",
    "tensor_test = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "print(tensor_train)\n",
    "print(tensor_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c99381-cb44-4926-956b-e497b4e4589a",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7325daf-f896-4057-bbec-73efbccab5b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     output  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(feature))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m---> 18\u001b[0m input_size  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     20\u001b[0m out_size    \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "class ECclassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, out_size):\n",
    "    super(ECclassifier, self).__init__()\n",
    "\n",
    "    self.hidden  = nn.Linear(input_size, hidden_size)\n",
    "    self.relu    = nn.ReLU()\n",
    "    self.out     = nn.Linear(hidden_size, out_size)\n",
    "    self.softmax = nn.Softmax()\n",
    "\n",
    "  def forward(self, X):\n",
    "    \n",
    "    feature = self.relu(self.hidden(X))\n",
    "    output  = self.softmax(self.out(feature))\n",
    "\n",
    "    return output\n",
    "\n",
    "input_size  = train_data.shape[1]\n",
    "hidden_size = 32\n",
    "out_size    = 7\n",
    "\n",
    "net = ECclassifier(input_size, hidden_size, out_size).to(device) #cast na GPU \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684ab102-55c0-4c86-b163-4090db457b9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2724958817.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Fluxo de Treinamento\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Fluxo de Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e11a8-64e0-4220-b3d7-60840105db7c",
   "metadata": {},
   "source": [
    "Fluxo de Validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63264b59-4f0f-4310-aacd-0f124a3030f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (2443592535.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[51], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    y'= {0.12; 0.09; 0.25; 0.14}\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "----> Função de perda de problema de classificação: \n",
    "\n",
    "y'= {0.12; 0.09; 0.25; 0.14}\n",
    "y= {0; 0; 1; 0} ----> dog \n",
    "\n",
    "- Cross Entropy (entropia cruzada) / Log Loss\n",
    "\n",
    "def CrossEntropy (yHat , y):\n",
    "    if y == 1:\n",
    "        return -log(yHat)\n",
    "    else: \n",
    "        return -log(1 - yHat)\n",
    "\n",
    "\n",
    "        ----> Medir loss ao longo das iteracoes para ver se modelo ta treinando bem ou nao \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c02e5c-62a6-4db4-b5b2-d81fd47d081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_to_enzyme_esm2: str, path_to_enzyme_splitX_csv: str):\n",
    "    \"\"\"\n",
    "    Reads in esm2 and splitX.csv\n",
    "    :param path_to_enzyme_esm2: Absolute path to enzyme_esm2_splitX\n",
    "    :param path_to_enzyme_splitX_csv: Absolute path to enzyme_splitX.csv\n",
    "    :return: A dataframe (representing splitX.csv), headers of esm2 (list), esm2 embeddings (list)\n",
    "    \"\"\"\n",
    "\n",
    "    headers = []\n",
    "    embeddings = []\n",
    "\n",
    "    with h5py.File(path_to_enzyme_esm2) as hdf_handle:\n",
    "        for header, emb in hdf_handle.items():\n",
    "            headers.append(header)\n",
    "            embeddings.append(np.array(list(emb)))\n",
    "    print(\"LOG: ESM2 DONE\")\n",
    "    \n",
    "    enzyme_csv = pd.read_csv(path_to_enzyme_splitX_csv, header=0, sep=',')\n",
    "    print(\"LOG: CSV DONE\")\n",
    "    print(\"Enzymes in SplitX.csv:\", len(enzyme_csv))\n",
    "    \n",
    "    # TODO: Inorder to apply this line of code, I also have to find the corresponding embeddings and \n",
    "    #  drop these as well...\n",
    "    # enzyme_csv = filter_unwanted_seqs(enzyme_csv, True)\n",
    "\n",
    "    # control\n",
    "    print(\"Embeddings:\", len(embeddings))\n",
    "    print(\"Embedding headers:\", len(headers))\n",
    "\n",
    "    return[enzyme_csv, headers, embeddings]\n",
    "\n",
    "def add_labels(enzymes: pd.DataFrame, entry_ids: list, esm2_emb: list):\n",
    "    \"\"\"\n",
    "    Takes in the output of read_data(), grabs the corresponding ec number out of splitX.csv (enzymes) and adds this label \n",
    "    to our esm2 embeddings\n",
    "    :param enzymes: Dataframe of splitX.csv\n",
    "    :param entry_ids: Headers of esm2 emeddings\n",
    "    :param esm2_emb: Embeddings\n",
    "    :return: A list containing tupels of (header, label, embedding)\n",
    "    \"\"\"\n",
    "\n",
    "    header_labels = {} # dict with → header(id): label (1-7)\n",
    "    missing_headers = set() # set containing missing headers\n",
    "\n",
    "    header_to_ec_mapping = {}\n",
    "    for entry, ec_number in zip(enzymes[\"Entry\"], enzymes[\"EC number\"]):\n",
    "        header_to_ec_mapping[entry] = ec_number[0]  # Extract the first character\n",
    "\n",
    "    for header in entry_ids:\n",
    "        number = enzymes.loc[enzymes[\"Entry\"] == header,\"EC number\"]\n",
    "        try:\n",
    "            ec = int(number.iat[0][0])-1 # since we always start counting from 0\n",
    "            header_labels[header] = ec\n",
    "        except IndexError:\n",
    "            missing_headers.add(header)\n",
    "\n",
    "    header_label_tuples = [(header, header_labels[header]) for header in entry_ids if header not in missing_headers]\n",
    "\n",
    "    # Create a new list of embeddings with labels\n",
    "    embeddings_with_labels = []\n",
    "\n",
    "    for header, emb in zip(entry_ids, esm2_emb):\n",
    "        curr_label = next((label for h, label in header_label_tuples if h == header), None)\n",
    "        if curr_label is not None:\n",
    "            embeddings_with_labels.append((header, curr_label, emb))\n",
    "\n",
    "    return embeddings_with_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
